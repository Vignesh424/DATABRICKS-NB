{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dc22a8e-fe4e-41b7-87d2-fc88df7b3078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+\n|    Name|Department|Salary|\n+--------+----------+------+\n|    Saif|        IT| 12000|\n|   Ankit|     Sales| 23000|\n|   Samay|     Sales| 23000|\n| Harshal| Logistics| 20000|\n|Lakshman| Logistics| 23000|\n+--------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "\n",
    "spark = SparkSession.builder.appName('DB').getOrCreate()\n",
    "df = [('Saif', 'IT', 12000),\n",
    "      ('Ankit', 'Sales', 23000),\n",
    "      ('Samay', 'Sales', 23000),\n",
    "      ('Harshal', 'Logistics', 20000),\n",
    "      ('Lakshman', 'Logistics', 23000)]\n",
    "columns = ['Name', 'Department', 'Salary']\n",
    "data = spark.createDataFrame(data=df, schema=columns)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a474029-0f1f-4b1d-a1a9-b5dd93655a62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Distinct**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13b01eaf-4adf-411a-9e63-76a91e59cf15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Count 5\n+------+----------+------+\n|  Name|Department|Salary|\n+------+----------+------+\n|  Saif|        IT| 12000|\n|Faisal|        IT| 14000|\n| Karen|  Research| 23000|\n|Yogesh|        HR| 34000|\n|  Saif|        IT| 12000|\n| Lalit|        HR| 23000|\n+------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName('Distinct').getOrCreate()\n",
    "data = [('Saif', 'IT', 12000), ('Faisal', 'IT',14000),\n",
    "        ('Karen','Research',23000), ('Yogesh','HR',34000),\n",
    "        ('Saif', 'IT', 12000), ('Lalit', 'HR',23000)]\n",
    "\n",
    "data = spark.createDataFrame(data=data, schema=columns)\n",
    "new_df = data.distinct().count()\n",
    "print('Distinct Count',new_df)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6975226a-bd27-4028-a18b-b286046adf46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**drop duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8db9391a-21fc-4d8a-b794-6dffbf0dd5f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+\n|  Name|Department|Salary|\n+------+----------+------+\n|  Saif|        IT| 12000|\n|Faisal|        IT| 14000|\n| Karen|  Research| 23000|\n|Yogesh|        HR| 34000|\n| Lalit|        HR| 23000|\n+------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName('Distinct').getOrCreate()\n",
    "data = [('Saif', 'IT', 12000), ('Faisal', 'IT',14000),\n",
    "        ('Karen','Research',23000), ('Yogesh','HR',34000),\n",
    "        ('Saif', 'IT', 12000), ('Lalit', 'HR',23000)]\n",
    "\n",
    "data = spark.createDataFrame(data=data, schema=columns)\n",
    "new_df = data\n",
    "new_df = data.dropDuplicates()\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59efcba1-07f1-4765-b4b3-592d2bf1a235",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**groupby**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3f500c7-690a-4120-b85a-921a2addb987",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+\n|  Name|Department|Salary|\n+------+----------+------+\n|  Saif|        IT| 12000|\n|Faisal|        IT| 14000|\n|Sachin|        IT| 23000|\n|Nikhil|        HR| 34000|\n| Kajal|  Research| 19000|\n| Sasha|        HR| 32000|\n| Karen|  Research| 23000|\n|Yogesh|        HR| 34000|\n| Lalit|        HR| 23000|\n+------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "\n",
    "spark = SparkSession.builder.appName('Groupby').getOrCreate()\n",
    "data = [('Saif', 'IT', 12000), ('Faisal', 'IT',14000),\n",
    "        ('Sachin','IT',23000), ('Nikhil','HR',34000),\n",
    "        ('Kajal','Research',19000), ('Sasha','HR',32000),\n",
    "        ('Karen','Research',23000), ('Yogesh','HR',34000),\n",
    "        ('Saif', 'IT', 12000), ('Lalit', 'HR',23000)]\n",
    "\n",
    "data = spark.createDataFrame(data=data, schema=columns)\n",
    "new_df = data\n",
    "new_df = data.dropDuplicates()\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1907645-de36-43e3-9d57-a98c13d340b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n|Department|sum(Salary)|\n+----------+-----------+\n|        IT|      61000|\n|        HR|     123000|\n|  Research|      42000|\n+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "new_df = data.groupby('Department').sum('Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83d241ce-1d53-4632-aea7-0406fc9dc1ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+\n|  Name|Department|Salary|\n+------+----------+------+\n|Nikhil|        HR| 34000|\n| Sasha|        HR| 32000|\n|Yogesh|        HR| 34000|\n| Lalit|        HR| 23000|\n+------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data.filter(col('Department')=='HR').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9386da3-cf4e-4609-ba74-059454a8e23c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+\n|  Name|Department|Salary|\n+------+----------+------+\n|  Saif|        IT| 12000|\n|Faisal|        IT| 14000|\n| Kajal|  Research| 19000|\n|  Saif|        IT| 12000|\n+------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data.filter(col('Salary')<=20000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f24830f-c71e-4937-be87-1efd642b88b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Database related Functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}