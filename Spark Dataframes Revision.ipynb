{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38b33b1a-7e62-48cb-ba26-ab5cca43cde7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Printing the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfdc90f2-82af-47f9-83f2-666d5593c0bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "spark = SparkSession.builder.appName(\"ExampleApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8ba8856-3e1d-4328-9f76-da5634a051f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- first_name: string (nullable = true)\n |-- middle_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+----------+-----------+---------+-----+------+------+\n|first_name|middle_name|last_name|dob  |gender|salary|\n+----------+-----------+---------+-----+------+------+\n|James     |           |Smith    |36636|M     |60000 |\n|Michael   |Rose       |         |40288|M     |70000 |\n|Robert    |           |Williams |42114|      |400000|\n|Maria     |Anne       |Jones    |39192|F     |500000|\n|Jen       |Mary       |Brown    |     |F     |0     |\n+----------+-----------+---------+-----+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#creating a dataset\n",
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n",
    "        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),\n",
    "        (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),\n",
    "        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),\n",
    "        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)]\n",
    "columns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\n",
    "pysparkDF = spark.createDataFrame(data = data, schema = columns)\n",
    "pysparkDF.printSchema()\n",
    "pysparkDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1804ab03-74e4-4694-9745-247531d03526",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ***SPARK FUNCTIONS***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6091d19-2d4b-4aaf-b722-f59eb6076339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**when**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9c6dffa-5906-4c1e-a097-d313a0bc8b20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+------+\n|First Name|Last Name|Gender|Salary|\n+----------+---------+------+------+\n|Ravi      |Dubey    |Male  |12000 |\n|Gururaj   |Pal      |Male  |20000 |\n|Tribuvana |Das      |Female|10500 |\n|Malathi   |Ramaswamy|Female|20000 |\n+----------+---------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import when\n",
    "spark = SparkSession.builder.appName(\"ExampleApp\").getOrCreate()\n",
    "\n",
    "data2 = [('Ravi', 'Dubey', 'Male', 12000),\n",
    "        ('Gururaj', 'Pal','Male',20000),\n",
    "        ('Tribuvana', 'Das', 'Female', 10500),\n",
    "        ('Malathi', 'Ramaswamy','Female',20000)\n",
    "        ]\n",
    "\n",
    "columns = ['First Name', 'Last Name', 'Gender', 'Salary']\n",
    "pysparkDF = spark.createDataFrame(data = data2, schema=columns)\n",
    "#pysparkDF.printSchema()\n",
    "pysparkDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a683d8b2-52cb-4bd0-8227-708bea853d9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+------+----------+\n|First Name|Last Name|Gender|Salary|Gender_Abb|\n+----------+---------+------+------+----------+\n|      Ravi|    Dubey|  Male| 12000|         M|\n|   Gururaj|      Pal|  Male| 20000|         M|\n| Tribuvana|      Das|Female| 10500|         F|\n|   Malathi|Ramaswamy|Female| 20000|         F|\n+----------+---------+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#Add new column Gen_Abb \n",
    "new_df = pysparkDF.withColumn(\"Gender_Abb\", when(col(\"Gender\")=='Male', 'M')\n",
    "                   .when(col(\"Gender\")=='Female',\"F\").otherwise(col(\"Gender\")))\n",
    "new_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0237b62b-8936-42ba-a27a-4e645eb74756",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**expr**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "218a669c-4960-4ebb-af30-c89d7e800784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+----------+\n|First_Name|Last_Name|Salary|Department|\n+----------+---------+------+----------+\n|     Sagar|     More| 12000|     Sales|\n|  Abhishek|    Dubey| 13000|     Sales|\n|    Sheela|     Vyas| 15000|        HR|\n|    Namita|  Godbole| 10000|        HR|\n|   Radhika|   Mathur| 23000|        IT|\n|    Antony|    Louis| 25000|        IT|\n+----------+---------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "import pyspark\n",
    "spark = SparkSession.builder.appName('Expr').getOrCreate()\n",
    "\n",
    "data = [('Sagar', 'More', 12000, 'Sales'),\n",
    "        ('Abhishek', 'Dubey', 13000, 'Sales'),\n",
    "        ('Sheela', 'Vyas', 15000, 'HR'),\n",
    "        ('Namita','Godbole', 10000, 'HR'),\n",
    "        ('Radhika', 'Mathur', 23000,'IT'),\n",
    "        ('Antony', 'Louis',25000, 'IT')]\n",
    "\n",
    "columns = ['First_Name', 'Last_Name', 'Salary', 'Department']\n",
    "new_data = spark.createDataFrame(data, columns)\n",
    "new_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35e5d386-9916-42ef-b14c-d96528e91287",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+----------+------+\n|First_Name|Last_Name|Salary|Department|Gender|\n+----------+---------+------+----------+------+\n|     Sagar|     More| 12000|     Sales|  Male|\n|  Abhishek|    Dubey| 13000|     Sales|  Male|\n|    Sheela|     Vyas| 15000|        HR|Female|\n|    Namita|  Godbole| 10000|        HR|Female|\n|   Radhika|   Mathur| 23000|        IT|Female|\n|    Antony|    Louis| 25000|        IT|  Male|\n+----------+---------+------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "new_case = new_data.withColumn('Gender',expr(\"CASE WHEN First_Name='Abhishek' or First_Name= 'Antony' or First_Name='Sagar' THEN 'Male' \"+\n",
    "                                             \"WHEN First_Name='Sheela' or First_Name= 'Radhika' or First_Name= 'Namita' THEN 'Female' \"+\n",
    "                                             \"ELSE 'Unknown' END\"))\n",
    "new_case.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b65d2f5-e60a-4bb6-b030-a9ac52de260a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+----------+------+\n|First_Name|Last_Name|Salary|Department|Degree|\n+----------+---------+------+----------+------+\n|     Sagar|     More| 12000|     Sales|   MBA|\n|  Abhishek|    Dubey| 13000|     Sales|   MBA|\n|    Sheela|     Vyas| 15000|        HR|    HR|\n|    Namita|  Godbole| 10000|        HR|    HR|\n|   Radhika|   Mathur| 23000|        IT|    IT|\n|    Antony|    Louis| 25000|        IT|    IT|\n+----------+---------+------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "new_case2 = new_data.withColumn('Degree', expr(\"CASE WHEN DEPARTMENT='Sales' THEN 'MBA' \"+\n",
    "                                               \"WHEN DEPARTMENT = 'HR' THEN 'HR' \"+\n",
    "                                               \"ELSE 'IT' END\" ))\n",
    "new_case2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9fdeac8-ad58-4e7c-bab6-a3093aedeaf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**concat_ws**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89d8a37e-3243-4b3a-851b-21acda44c317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n| Name|        Designations|Salary|\n+-----+--------------------+------+\n|  Roy|[Data Engineer, D...| 12000|\n|Rashi|[Data Analyst, Da...| 15000|\n| Ravi|[Data Analyst, Da...| 18000|\n+-----+--------------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark\n",
    "\n",
    "spark = SparkSession.builder.appName('Concat_WS').getOrCreate()\n",
    "\n",
    "data = [('Roy', ['Data Engineer', 'Data Scientist'],12000),\n",
    "              ('Rashi', ['Data Analyst', 'Data Scientist'],15000),\n",
    "              ('Ravi', ['Data Analyst', 'Data Scientist'],18000)\n",
    "              ]\n",
    "columns = ['Name', 'Designations', 'Salary']\n",
    "\n",
    "data_conws = spark.createDataFrame(data, columns)\n",
    "data_conws.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03c636f3-843f-4c2d-85b3-2de5d119817f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n| Name|        Designations|Salary|\n+-----+--------------------+------+\n|  Roy|Data Engineer,Dat...| 12000|\n|Rashi|Data Analyst,Data...| 15000|\n| Ravi|Data Analyst,Data...| 18000|\n+-----+--------------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#Designation Array to be separated to String\n",
    "from pyspark.sql.functions import *\n",
    "new_data = data_conws.withColumn('Designations',\n",
    "                                 concat_ws(',',col('Designations')\n",
    "                                 ))\n",
    "new_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd377fe6-129b-4e87-90b7-eb7aeba21d43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#***Math Functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6adc326d-9276-4a9a-a681-edd4c05c951b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#sum\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark\n",
    "\n",
    "spark = SparkSession.builder.appName('Math_Functions').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "763b5164-5388-43a3-a0a2-61c1d95843dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n+-------------+----------+-----+------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "data_math = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "columns = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "stat_data = spark.createDataFrame(data_math,columns)\n",
    "stat_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "559050b9-89b1-4082-a8f6-4fba1cf2390b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**sum()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe933e30-6554-4f18-bb2b-00d3f1f78033",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum: 779000\n"
     ]
    }
   ],
   "source": [
    "#sum\n",
    "print(\"Sum: \" + str(stat_data.select(sum(\"Salary\")).collect()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "839675c2-8338-4462-8529-c99f1307dd9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg: 86555.55555555556\n"
     ]
    }
   ],
   "source": [
    "#Avg\n",
    "print(\"Avg: \" + str(stat_data.select(mean(\"Salary\")).collect()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2290d1a1-40b6-4d13-801a-02a6ae2b48f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 79000\nMax: 99000\n"
     ]
    }
   ],
   "source": [
    "#Min and Max\n",
    "print(\"Min: \" + str(stat_data.select(min(\"Salary\")).collect()[0][0]))\n",
    "print(\"Max: \"+ str(stat_data.select(max('Salary')).collect()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d976809f-4cd0-4666-9e15-5ae6a72317c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STD: 6540.472290116195\n"
     ]
    }
   ],
   "source": [
    "#Standard Deviation\n",
    "print(\"STD: \"+ str(stat_data.select(stddev('Salary')).collect()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "631a121c-1d21-466e-8d34-4dd8ca0aba99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Var: 42777777.77777778\n"
     ]
    }
   ],
   "source": [
    "#Variance\n",
    "print(\"Var: \"+ str(stat_data.select(variance('Salary')).collect()[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40020f1a-1ddd-47a5-b68f-c53ab980c83b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#***Window Functions***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02d38c49-0791-404d-8a24-5c2703a1fdce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**row-number**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32a2a248-6c72-4eb1-8565-32489b110a2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+\n|   Name| Dept|Salary|\n+-------+-----+------+\n|   John|Sales| 12000|\n|Harshal|   IT| 24000|\n|Farsana|   HR| 34000|\n| Rashmi| Tech| 32000|\n+-------+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.window import Window \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName('Window').getOrCreate()\n",
    "\n",
    "data = [('John', 'Sales', 12000), ('Harshal', 'IT', 24000), ('Farsana', 'HR',34000), ('Rashmi', 'Tech', 32000)]\n",
    "columns = ['Name', 'Dept', 'Salary']\n",
    "window_data = spark.createDataFrame(data, columns)\n",
    "if window_data is not None: \n",
    "  # Define window specification \n",
    "  window_data.show()\n",
    "else:\n",
    "  print(\"DataFrame is None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f3e1339-9f9a-4866-9af6-13787607389c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Row Number**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "739338b0-2c47-471a-90a5-1570b368a116",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+----------+\n|   Name| Dept|Salary|Row Number|\n+-------+-----+------+----------+\n|Farsana|   HR| 34000|         1|\n|Harshal|   IT| 24000|         1|\n|   John|Sales| 12000|         1|\n| Rashmi| Tech| 32000|         1|\n+-------+-----+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window \n",
    "from pyspark.sql.functions import rank, dense_rank,row_number, lag, lead\n",
    "if window_data is not None: \n",
    "  # Define window specification \n",
    "  windows = Window.partitionBy(\"Name\").orderBy(\"Salary\") \n",
    "  # Add Row column \n",
    "  row_data = window_data.withColumn(\"Row Number\", row_number().over(windows)) \n",
    "  row_data.show()\n",
    "else:\n",
    "  print(\"DataFrame is None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f63be7a1-3a72-4fac-9f8d-d25e66ef84c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Rank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f18d3d9c-4df3-40b0-a42a-73cbcfbe20e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+----+\n|   Name| Dept|Salary|Rank|\n+-------+-----+------+----+\n|Farsana|   HR| 34000|   1|\n|Harshal|   IT| 24000|   1|\n|   John|Sales| 12000|   1|\n| Rashmi| Tech| 32000|   1|\n+-------+-----+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if window_data is not None: \n",
    "  # Define window specification \n",
    "  windows = Window.partitionBy(\"Dept\").orderBy(\"Salary\") \n",
    "  # Add Rank column \n",
    "  rank_data = window_data.withColumn(\"Rank\", rank().over(windows)) \n",
    "  rank_data.show()\n",
    "else:\n",
    "  print(\"DataFrame is None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f8bbf9d-a5ab-4960-93cd-c1291c134e86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Dense Rank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dea5d2f6-2348-4dfb-bc43-7ad5a2178be0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+----------+\n|   Name| Dept|Salary|Dense_Rank|\n+-------+-----+------+----------+\n|Farsana|   HR| 34000|         1|\n|Harshal|   IT| 24000|         1|\n|   John|Sales| 12000|         1|\n| Rashmi| Tech| 32000|         1|\n+-------+-----+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "if window_data is not None: \n",
    "  # Define window specification \n",
    "  windows = Window.partitionBy(\"Dept\").orderBy(\"Salary\") \n",
    "  # Add Dense_Rank column \n",
    "  drank_data = window_data.withColumn(\"Dense_Rank\", dense_rank().over(windows)) \n",
    "  drank_data.show()\n",
    "else:\n",
    "  print(\"DataFrame is None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b70d8d0-49d9-4a00-83c1-bff95ba0563f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Lag**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0abc2909-a6b4-41ef-a05a-a0a476b47b87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+----+\n|   Name| Dept|Salary| lag|\n+-------+-----+------+----+\n|Farsana|   HR| 34000|null|\n|Harshal|   IT| 24000|null|\n|   John|Sales| 12000|null|\n| Rashmi| Tech| 32000|null|\n+-------+-----+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lag\n",
    "\n",
    "window_data.withColumn(\"lag\",lag(\"Salary\",2).over(windows)) \\\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9678cdb7-8a8c-4ef0-a363-9a3787d9304c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Lead**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acff9e5e-c06f-451d-8a14-5cf266f3f827",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+----+\n|   Name| Dept|Salary|lead|\n+-------+-----+------+----+\n|Farsana|   HR| 34000|null|\n|Harshal|   IT| 24000|null|\n|   John|Sales| 12000|null|\n| Rashmi| Tech| 32000|null|\n+-------+-----+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead \n",
    "window_data.withColumn(\"lead\",lead(\"Salary\",2).over(windows)) \\\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2a07f91-f27d-4616-a336-202e3f69940e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [
    {
     "elements": [
      {
       "dashboardResultIndex": null,
       "elementNUID": "0237b62b-8936-42ba-a27a-4e645eb74756",
       "elementType": "command",
       "guid": "1fc154ef-39dd-43d2-8a0b-28487decb960",
       "options": null,
       "position": {
        "height": 1,
        "width": 12,
        "x": 0,
        "y": 18,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": null,
       "elementNUID": "38b33b1a-7e62-48cb-ba26-ab5cca43cde7",
       "elementType": "command",
       "guid": "30affa26-9b47-465b-9198-75219fb0eeb8",
       "options": null,
       "position": {
        "height": 1,
        "width": 12,
        "x": 0,
        "y": 0,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": null,
       "elementNUID": "e9fdeac8-ad58-4e7c-bab6-a3093aedeaf8",
       "elementType": "command",
       "guid": "8091a065-97b2-49d8-a967-23cfbfac3a38",
       "options": null,
       "position": {
        "height": 1,
        "width": 12,
        "x": 0,
        "y": 24,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": null,
       "elementNUID": "c6091d19-2d4b-4aaf-b722-f59eb6076339",
       "elementType": "command",
       "guid": "8eb0d27e-2d37-4243-ab04-5adee36a8a85",
       "options": null,
       "position": {
        "height": 1,
        "width": 12,
        "x": 0,
        "y": 12,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": null,
       "elementNUID": "1804ab03-74e4-4694-9745-247531d03526",
       "elementType": "command",
       "guid": "f1351c94-396f-4015-8822-3603769eab1d",
       "options": null,
       "position": {
        "height": 2,
        "width": 12,
        "x": 0,
        "y": 6,
        "z": null
       },
       "resultIndex": null
      }
     ],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "675e662f-fd56-42ba-bad5-21a9b7d8ca35",
     "origId": 3657766310503532,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Spark Dataframes Revision",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}